{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36ccd7cb",
   "metadata": {},
   "source": [
    "### This notebook performs text cleaning and preprocessing and runs two models to predict whether an article is reliable (real) or unreliable (fake)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "-4kWPkjsy28R",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 89945,
     "status": "ok",
     "timestamp": 1684801243530,
     "user": {
      "displayName": "Carolyn Liu",
      "userId": "09235749092370364127"
     },
     "user_tz": 300
    },
    "id": "-4kWPkjsy28R",
    "outputId": "247924d1-2004-403b-b332-d803e590f802"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 310.8/310.8 MB 3.5 MB/s eta 0:00:00\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 489.4/489.4 kB 26.7 MB/s eta 0:00:00\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "apt-get update -qq\n",
    "apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n",
    "wget -q \"https://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop2.7.tgz\" > /dev/null\n",
    "tar -xvf spark-3.1.1-bin-hadoop2.7.tgz > /dev/null\n",
    "\n",
    "pip install pyspark findspark --quiet\n",
    "pip install sparknlp --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3437bd8",
   "metadata": {
    "executionInfo": {
     "elapsed": 1599,
     "status": "ok",
     "timestamp": 1684801245117,
     "user": {
      "displayName": "Carolyn Liu",
      "userId": "09235749092370364127"
     },
     "user_tz": 300
    },
    "id": "b3437bd8"
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from pyspark.sql.functions import array, col, udf, split\n",
    "from pyspark.ml import Pipeline\n",
    "from sparknlp.annotator import Lemmatizer, LemmatizerModel, Tokenizer, StopWordsCleaner, Normalizer\n",
    "from sparknlp.base import DocumentAssembler, Finisher\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression, GBTClassifier\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "import pyspark.ml.evaluation as evals\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c44c787e",
   "metadata": {
    "executionInfo": {
     "elapsed": 117277,
     "status": "ok",
     "timestamp": 1684801390199,
     "user": {
      "displayName": "Carolyn Liu",
      "userId": "09235749092370364127"
     },
     "user_tz": 300
    },
    "id": "c44c787e"
   },
   "outputs": [],
   "source": [
    "# setup Spark\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "# Find Spark so that we can access session within our notebook\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# Start SparkSession on all available cores\n",
    "from pyspark.sql import SparkSession\n",
    "# spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\")\\\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:4.3.2\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f67e38c",
   "metadata": {
    "executionInfo": {
     "elapsed": 5933,
     "status": "ok",
     "timestamp": 1684802040030,
     "user": {
      "displayName": "Carolyn Liu",
      "userId": "09235749092370364127"
     },
     "user_tz": 300
    },
    "id": "0f67e38c"
   },
   "outputs": [],
   "source": [
    "# read in data\n",
    "data = spark.read.csv('train1.csv',\n",
    "                      header='true',\n",
    "                      inferSchema='true',\n",
    "                      multiLine=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "393c26b4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3397,
     "status": "ok",
     "timestamp": 1684802043423,
     "user": {
      "displayName": "Carolyn Liu",
      "userId": "09235749092370364127"
     },
     "user_tz": 300
    },
    "id": "393c26b4",
    "outputId": "42b1362c-7d34-4830-84e1-82d90ad57aa6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Columns: 5\n",
      "Total Rows: 6288\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- author: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Total Columns: %d' % len(data.dtypes))\n",
    "print('Total Rows: %d' % data.count())\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59bfe2c8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2337,
     "status": "ok",
     "timestamp": 1684802045758,
     "user": {
      "displayName": "Carolyn Liu",
      "userId": "09235749092370364127"
     },
     "user_tz": 300
    },
    "id": "59bfe2c8",
    "outputId": "ddad3d32-c697-4571-acf7-686cbe7e9229"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------------+-----+\n",
      "| id|               title|              author|                text|label|\n",
      "+---+--------------------+--------------------+--------------------+-----+\n",
      "|  0|House Dem Aide: W...|       Darrell Lucus|\"House Dem Aide: ...|    1|\n",
      "|  1|FLYNN: Hillary Cl...|     Daniel J. Flynn|Ever get the feel...|    0|\n",
      "|  2|Why the Truth Mig...|  Consortiumnews.com|Why the Truth Mig...|    1|\n",
      "|  3|15 Civilians Kill...|     Jessica Purkiss|Videos 15 Civilia...|    1|\n",
      "|  4|Iranian woman jai...|      Howard Portnoy|Print An Iranian ...|    1|\n",
      "|  5|Jackie Mason: Hol...|     Daniel Nussbaum|In these trying t...|    0|\n",
      "|  6|Life: Life Of Lux...|                null|Ever wonder how B...|    1|\n",
      "|  7|Benoît Hamon Wins...|     Alissa J. Rubin|PARIS  —   France...|    0|\n",
      "|  8|Excerpts From a D...|                null|Donald J. Trump i...|    0|\n",
      "|  9|A Back-Channel Pl...|Megan Twohey and ...|A week before Mic...|    0|\n",
      "| 10|Obama’s Organizin...|         Aaron Klein|Organizing for Ac...|    0|\n",
      "| 11|\"BBC Comedy Sketc...|     Chris Tomlinson|The BBC produced ...|    0|\n",
      "| 12|Russian Researche...|       Amando Flavio|The mystery surro...|    1|\n",
      "| 13|US Officials See ...|          Jason Ditz|Clinton Campaign ...|    1|\n",
      "| 14|Re: Yes There Are...|        AnotherAnnie|Yes There Are Pai...|    1|\n",
      "| 15|In Major League S...|       Jack Williams|Guillermo Barros ...|    0|\n",
      "| 16|Wells Fargo Chief...|Michael Corkery a...|The scandal engul...|    0|\n",
      "| 17|Anonymous Donor P...|            Starkman|A Caddo Nation tr...|    1|\n",
      "| 18|FBI Closes In On ...|             The Doc|FBI Closes In On ...|    1|\n",
      "| 19|Chuck Todd: ’Buzz...|           Jeff Poor|Wednesday after  ...|    0|\n",
      "+---+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166b2e5b",
   "metadata": {
    "id": "166b2e5b"
   },
   "source": [
    "### Check Class Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16d9670a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1574,
     "status": "ok",
     "timestamp": 1684802047329,
     "user": {
      "displayName": "Carolyn Liu",
      "userId": "09235749092370364127"
     },
     "user_tz": 300
    },
    "id": "16d9670a",
    "outputId": "5686f982-341b-464a-9925-95190217ecd3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "| null|    1|\n",
      "|    1| 3386|\n",
      "|    0| 3357|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "labels = (data.groupBy('label')\n",
    "             .count()\n",
    "        )\n",
    "labels.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cd0b6f",
   "metadata": {
    "id": "17cd0b6f"
   },
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd929335",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1684802055553,
     "user": {
      "displayName": "Carolyn Liu",
      "userId": "09235749092370364127"
     },
     "user_tz": 300
    },
    "id": "bd929335"
   },
   "outputs": [],
   "source": [
    "def doc_assembler(inputCol):\n",
    "    '''Spark NLP document assembler'''\n",
    "    \n",
    "    return DocumentAssembler().setInputCol(inputCol)\n",
    "\n",
    "\n",
    "def tokenizer(inputCol, outputCol):\n",
    "    '''Tokenize text for input to the lemmatizer'''\n",
    "    \n",
    "    tokenizer = (Tokenizer()\n",
    "        .setInputCols([inputCol])\n",
    "        .setOutputCol(outputCol))\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def stopwords(inputCol, outputCol):\n",
    "    '''Remove stopwords'''\n",
    "\n",
    "    stopwords = StopWordsCleaner.pretrained(\"stopwords_en\", \"en\") \\\n",
    "        .setInputCols([inputCol]) \\\n",
    "        .setOutputCol(outputCol)\n",
    "    return stopwords\n",
    "\n",
    "\n",
    "def normalizer(inputCol, outputCol):\n",
    "    '''Remove unnecessary characters and make tokens lowercase'''\n",
    "    \n",
    "    normalizer = (Normalizer() \n",
    "        .setInputCols([inputCol])\n",
    "        .setOutputCol(outputCol)\n",
    "        .setLowercase(True))\n",
    "    return normalizer\n",
    "          \n",
    "    \n",
    "def lemmatizer(inputCol, outputCol):\n",
    "    '''\n",
    "    Retrieve root words out of the input tokens\n",
    "    using a pretrained lemmatizer\n",
    "    '''\n",
    "    \n",
    "    lemmatizer = (LemmatizerModel.pretrained(name=\"lemma_antbnc\", lang=\"en\")\n",
    "        .setInputCols([inputCol])\n",
    "        .setOutputCol(outputCol))\n",
    "    return lemmatizer\n",
    "\n",
    "\n",
    "def finisher(finishedCol):\n",
    "    '''Finisher transform for Spark NLP pipeline'''\n",
    "    \n",
    "    finisher = (Finisher()\n",
    "        .setInputCols([finishedCol])\n",
    "        .setIncludeMetadata(False))\n",
    "    return finisher\n",
    "\n",
    "\n",
    "def run_sparknlp_pipeline(df):\n",
    "    '''\n",
    "    Create a SparkNLP pipeline that takes the input df to produce a final output\n",
    "    column storing each document as a sequence of lemmas (root words)\n",
    "    '''\n",
    "   \n",
    "    nlpPipeline = Pipeline(stages=[\n",
    "        doc_assembler(\"text\"),\n",
    "        tokenizer(\"document\", \"token\"),\n",
    "        stopwords('token', 'token_s'),\n",
    "        normalizer('token_s', 'cleaned_tokens'),\n",
    "        lemmatizer(\"cleaned_tokens\", \"lemma\"),\n",
    "        finisher(\"lemma\")\n",
    "    ])\n",
    "    df1 = nlpPipeline.fit(df).transform(df).withColumnRenamed('finished_lemma', 'allTokens')\n",
    "\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "819e2bb3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10521,
     "status": "ok",
     "timestamp": 1684802331731,
     "user": {
      "displayName": "Carolyn Liu",
      "userId": "09235749092370364127"
     },
     "user_tz": 300
    },
    "id": "819e2bb3",
    "outputId": "6a7b1eba-4a78-4f48-a083-e030a3c56e18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopwords_en download started this may take some time.\n",
      "Approximate size to download 2.9 KB\n",
      "[OK!]\n",
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "data = data.dropna()\n",
    "sampled_data = data.sample(fraction=0.3)\n",
    "\n",
    "nlpPipelineDF = run_sparknlp_pipeline(sampled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4a13dcb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 762,
     "status": "ok",
     "timestamp": 1684802347961,
     "user": {
      "displayName": "Carolyn Liu",
      "userId": "09235749092370364127"
     },
     "user_tz": 300
    },
    "id": "c4a13dcb",
    "outputId": "47235257-2f48-497b-8ca9-970fc96755ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------------+-----+--------------------+\n",
      "| id|               title|              author|                text|label|           allTokens|\n",
      "+---+--------------------+--------------------+--------------------+-----+--------------------+\n",
      "|  2|Why the Truth Mig...|  Consortiumnews.com|Why the Truth Mig...|    1|[truth, fire, oct...|\n",
      "|  3|15 Civilians Kill...|     Jessica Purkiss|Videos 15 Civilia...|    1|[video, civilian,...|\n",
      "|  5|Jackie Mason: Hol...|     Daniel Nussbaum|In these trying t...|    0|[time, jackie, ma...|\n",
      "|  7|Benoît Hamon Wins...|     Alissa J. Rubin|PARIS  —   France...|    0|[paris, france, c...|\n",
      "| 13|US Officials See ...|          Jason Ditz|Clinton Campaign ...|    1|[clinton, campaig...|\n",
      "| 15|In Major League S...|       Jack Williams|Guillermo Barros ...|    0|[guillermo, barro...|\n",
      "| 18|FBI Closes In On ...|             The Doc|FBI Closes In On ...|    1|[fbi, close, hill...|\n",
      "| 19|Chuck Todd: ’Buzz...|           Jeff Poor|Wednesday after  ...|    0|[wednesday, donal...|\n",
      "| 25|Nukes and the UN:...|         Ira Helfand|Email In an histo...|    1|[email, historic,...|\n",
      "| 40|MRI Shows Detaile...|     Dr. Susan Berry|A newly developed...|    0|[newly, develop, ...|\n",
      "| 44|Conservatives Urg...|           Ian Mason|A group of attorn...|    0|[group, attorney,...|\n",
      "| 46|Press TV Debate: ...|Gordon Duff, Seni...|By Gordon Duff Se...|    1|[gordon, duff, se...|\n",
      "| 50|Sparking An Inner...|     Lance Schuttler|0 0 With humanity...|    1|[humanitys, awake...|\n",
      "| 63|Massive Anti-Trum...|Truth Broadcast N...|\"17 mins ago 2 Vi...|    1|[min, ago, view, ...|\n",
      "| 64|Review: ‘Lion’ Br...|         A. O. Scott|The first part of...|    0|[part, lion, gart...|\n",
      "| 66|Jury finds all Or...|               Admin|Oregon Live – by ...|    1|[oregon, live, ma...|\n",
      "| 70|How To Make Briqu...|         Chris Black|  22 2016 How To ...|    1|[make, briquette,...|\n",
      "| 72|Dress Like a Woma...|        Jacey Fortin|What does it mean...|    0|[dress, woman, so...|\n",
      "| 75|Democrats Have a ...|           Josh Katz|The Upshot’s new ...|    0|[upshot, senate, ...|\n",
      "| 86|              Poison|   Heather Callaghan|By Dr. Mark Sircu...|    1|[dr, mark, sircus...|\n",
      "+---+--------------------+--------------------+--------------------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nlpPipelineDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7e111e53",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1400,
     "status": "ok",
     "timestamp": 1684802353639,
     "user": {
      "displayName": "Carolyn Liu",
      "userId": "09235749092370364127"
     },
     "user_tz": 300
    },
    "id": "7e111e53",
    "outputId": "9e4db297-7282-4508-a40b-168d6fc7cde6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Rows: 5468\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[id: int, title: string, author: string, text: string, label: int, allTokens: array<string>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Total Rows: %d' % nlpPipelineDF.count())\n",
    "nlpPipelineDF.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8492f55e",
   "metadata": {
    "id": "8492f55e"
   },
   "source": [
    "### ML Prediction Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e8fa16a",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1684802353639,
     "user": {
      "displayName": "Carolyn Liu",
      "userId": "09235749092370364127"
     },
     "user_tz": 300
    },
    "id": "2e8fa16a"
   },
   "outputs": [],
   "source": [
    "def count_vec(inputCol, outputCol, params):\n",
    "    '''\n",
    "    Convert a collection of text documents to vectors of token counts\n",
    "    '''\n",
    "    cv = CountVectorizer(\n",
    "        inputCol=inputCol,\n",
    "        outputCol=outputCol,\n",
    "        vocabSize=params['vocabsize'],\n",
    "        minDF=params['minDF'],\n",
    "        maxDF=params['maxDF'],\n",
    "        minTF=1.0\n",
    "    )\n",
    "    return cv\n",
    "\n",
    "\n",
    "def ml_pipeline(df, params):\n",
    "    '''\n",
    "    Create a Spark ML pipeline and transform the input NLP-transformed DataFrame \n",
    "    to produce an features for an ML model\n",
    "    '''\n",
    "\n",
    "    mlPipeline = Pipeline(\n",
    "        stages=[\n",
    "            count_vec(\"allTokens\", \"features\", params)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    final_df = mlPipeline.fit(df).transform(df)\n",
    "\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3737bc7c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1684802355486,
     "user": {
      "displayName": "Carolyn Liu",
      "userId": "09235749092370364127"
     },
     "user_tz": 300
    },
    "id": "3737bc7c",
    "outputId": "39b11622-e54c-4654-e49a-567be9c994ee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vocabsize': 7000, 'minDF': 0.02, 'maxDF': 0.8}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_params = dict(vocabsize = 7000,\n",
    "    minDF = 0.02,\n",
    "    maxDF = 0.8\n",
    " )\n",
    "ml_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3abe0114",
   "metadata": {
    "executionInfo": {
     "elapsed": 71263,
     "status": "ok",
     "timestamp": 1684802429127,
     "user": {
      "displayName": "Carolyn Liu",
      "userId": "09235749092370364127"
     },
     "user_tz": 300
    },
    "id": "3abe0114"
   },
   "outputs": [],
   "source": [
    "final_df = ml_pipeline(nlpPipelineDF, ml_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7c909379",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1684802429128,
     "user": {
      "displayName": "Carolyn Liu",
      "userId": "09235749092370364127"
     },
     "user_tz": 300
    },
    "id": "7c909379",
    "outputId": "401d176f-2a1d-4798-c6d0-2a04ab70b77f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5468"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1ebe14",
   "metadata": {
    "id": "2a1ebe14"
   },
   "source": [
    "### Prepare for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cd78af7f",
   "metadata": {
    "executionInfo": {
     "elapsed": 1224,
     "status": "ok",
     "timestamp": 1684802430348,
     "user": {
      "displayName": "Carolyn Liu",
      "userId": "09235749092370364127"
     },
     "user_tz": 300
    },
    "id": "cd78af7f"
   },
   "outputs": [],
   "source": [
    "features = ['features']\n",
    "assembler = VectorAssembler(inputCols = features, outputCol = 'final_features', handleInvalid='skip')\n",
    "mlPipelineDF = assembler.transform(final_df)\n",
    "model_df = mlPipelineDF.select(['final_features', 'label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f58813f",
   "metadata": {
    "id": "8f58813f"
   },
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1e86bb51",
   "metadata": {
    "executionInfo": {
     "elapsed": 345881,
     "status": "ok",
     "timestamp": 1684802776226,
     "user": {
      "displayName": "Carolyn Liu",
      "userId": "09235749092370364127"
     },
     "user_tz": 300
    },
    "id": "1e86bb51"
   },
   "outputs": [],
   "source": [
    "# train-test split & hyperparameter tuning\n",
    "\n",
    "train, test = model_df.randomSplit([0.8, 0.2])\n",
    "lr = LogisticRegression(featuresCol='final_features', labelCol='label')\n",
    "\n",
    "grid = ParamGridBuilder().addGrid(lr.regParam, np.arange(0,\n",
    "    .1, .01)).addGrid(lr.elasticNetParam, [0, 1.0]).build()\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='label', metricName='areaUnderROC')\n",
    "cv = CrossValidator(estimator=lr, estimatorParamMaps=grid, evaluator=evaluator,\n",
    "     numFolds=5)\n",
    "\n",
    "# run model & evaluate\n",
    "cvModel = cv.fit(train)\n",
    "\n",
    "lr_pred = cvModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f450a514",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5439,
     "status": "ok",
     "timestamp": 1684802931876,
     "user": {
      "displayName": "Carolyn Liu",
      "userId": "09235749092370364127"
     },
     "user_tz": 300
    },
    "id": "f450a514",
    "outputId": "778f2862-858e-4fee-9a89-6412b11f303d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AUC:  0.9732311189506937\n"
     ]
    }
   ],
   "source": [
    "print(\"Test AUC: \", evaluator.evaluate(lr_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aa61c0ac",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4652,
     "status": "ok",
     "timestamp": 1684802939955,
     "user": {
      "displayName": "Carolyn Liu",
      "userId": "09235749092370364127"
     },
     "user_tz": 300
    },
    "id": "aa61c0ac",
    "outputId": "e7ca4881-d553-4917-c7ae-a79eec09c4af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|     avg(accuracy)|\n",
      "+------------------+\n",
      "|0.9216757741347905|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute accuracy\n",
    "lr_pred = lr_pred.withColumn('accuracy', (lr_pred.label == lr_pred.prediction).cast('float'))\n",
    "lr_pred.select(F.mean('accuracy')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860a468b",
   "metadata": {
    "id": "860a468b"
   },
   "source": [
    "### Tree-based Model (Gradient Boosted tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "360410e5",
   "metadata": {
    "executionInfo": {
     "elapsed": 45274,
     "status": "ok",
     "timestamp": 1684803990313,
     "user": {
      "displayName": "Carolyn Liu",
      "userId": "09235749092370364127"
     },
     "user_tz": 300
    },
    "id": "360410e5"
   },
   "outputs": [],
   "source": [
    "gbt = GBTClassifier(featuresCol='final_features', labelCol='label')\n",
    "\n",
    "tree_grid = ParamGridBuilder()\\\n",
    "  .addGrid(gbt.maxDepth, [2, 5])\\\n",
    "  .addGrid(gbt.maxIter, [10, 100])\\\n",
    "  .build()\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='label', metricName='areaUnderROC')\n",
    "\n",
    "cv = CrossValidator(estimator=gbt, estimatorParamMaps=tree_grid, evaluator=evaluator,\n",
    "     numFolds=5)\n",
    "\n",
    "# run model\n",
    "cvModel = cv.fit(train)\n",
    "\n",
    "tree_pred = cvModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5b8593f9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5062,
     "status": "ok",
     "timestamp": 1684804039943,
     "user": {
      "displayName": "Carolyn Liu",
      "userId": "09235749092370364127"
     },
     "user_tz": 300
    },
    "id": "5b8593f9",
    "outputId": "b822b67c-2ee7-498d-affe-55ac7166df01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AUC:  0.9724048148235575\n",
      "+-----------------+\n",
      "|    avg(accuracy)|\n",
      "+-----------------+\n",
      "|0.907103825136612|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate model\n",
    "\n",
    "print(\"Test AUC: \", evaluator.evaluate(tree_pred))\n",
    "\n",
    "# Compute accuracy\n",
    "tree_pred = tree_pred.withColumn('accuracy', (tree_pred.label == tree_pred.prediction).cast('float'))\n",
    "tree_pred.select(F.mean('accuracy')).show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
