{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "231bb3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/05/20 19:02:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# # Find Spark so that we can access session within our notebook\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# Start SparkSession on all available cores\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6202e647",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data = spark.read.csv('data/train1.csv',\n",
    "                      header='true',\n",
    "                      inferSchema='true',\n",
    "                      multiLine=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f24feeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Columns: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Rows: 20800\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- author: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print('Total Columns: %d' % len(data.dtypes))\n",
    "print('Total Rows: %d' % data.count())\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f99072fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------------+-----+\n",
      "| id|               title|              author|                text|label|\n",
      "+---+--------------------+--------------------+--------------------+-----+\n",
      "|  0|House Dem Aide: W...|       Darrell Lucus|\"House Dem Aide: ...|    1|\n",
      "|  1|FLYNN: Hillary Cl...|     Daniel J. Flynn|Ever get the feel...|    0|\n",
      "|  2|Why the Truth Mig...|  Consortiumnews.com|Why the Truth Mig...|    1|\n",
      "|  3|15 Civilians Kill...|     Jessica Purkiss|Videos 15 Civilia...|    1|\n",
      "|  4|Iranian woman jai...|      Howard Portnoy|Print An Iranian ...|    1|\n",
      "|  5|Jackie Mason: Hol...|     Daniel Nussbaum|In these trying t...|    0|\n",
      "|  6|Life: Life Of Lux...|                null|Ever wonder how B...|    1|\n",
      "|  7|Benoît Hamon Wins...|     Alissa J. Rubin|PARIS  —   France...|    0|\n",
      "|  8|Excerpts From a D...|                null|Donald J. Trump i...|    0|\n",
      "|  9|A Back-Channel Pl...|Megan Twohey and ...|A week before Mic...|    0|\n",
      "| 10|Obama’s Organizin...|         Aaron Klein|Organizing for Ac...|    0|\n",
      "| 11|\"BBC Comedy Sketc...|     Chris Tomlinson|The BBC produced ...|    0|\n",
      "| 12|Russian Researche...|       Amando Flavio|The mystery surro...|    1|\n",
      "| 13|US Officials See ...|          Jason Ditz|Clinton Campaign ...|    1|\n",
      "| 14|Re: Yes There Are...|        AnotherAnnie|Yes There Are Pai...|    1|\n",
      "| 15|In Major League S...|       Jack Williams|Guillermo Barros ...|    0|\n",
      "| 16|Wells Fargo Chief...|Michael Corkery a...|The scandal engul...|    0|\n",
      "| 17|Anonymous Donor P...|            Starkman|A Caddo Nation tr...|    1|\n",
      "| 18|FBI Closes In On ...|             The Doc|FBI Closes In On ...|    1|\n",
      "| 19|Chuck Todd: ’Buzz...|           Jeff Poor|Wednesday after  ...|    0|\n",
      "+---+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb267b4",
   "metadata": {},
   "source": [
    "### Check Class Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81c1de41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 6:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    1|10413|\n",
      "|    0|10387|\n",
      "+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "labels = (data.groupBy('label')\n",
    "             .count()\n",
    "        )\n",
    "labels.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a0c482",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd6eef9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Word2Vec, Word2VecModel\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer, StopWordsRemover, IDF, CountVectorizer\n",
    "from pyspark.sql.functions import regexp_replace, array, col, udf, split\n",
    "from pyspark.ml import Pipeline\n",
    "# from sparknlp.annotator import Lemmatizer\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.base import *\n",
    "from pyspark.ml.clustering import LDA\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e6ff2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 9:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------------+-----+--------------------+\n",
      "| id|               title|              author|                text|label|        text_cleaned|\n",
      "+---+--------------------+--------------------+--------------------+-----+--------------------+\n",
      "|  0|House Dem Aide: W...|       Darrell Lucus|\"House Dem Aide: ...|    1|House Dem Aide We...|\n",
      "|  1|FLYNN: Hillary Cl...|     Daniel J. Flynn|Ever get the feel...|    0|Ever get the feel...|\n",
      "|  2|Why the Truth Mig...|  Consortiumnews.com|Why the Truth Mig...|    1|Why the Truth Mig...|\n",
      "|  3|15 Civilians Kill...|     Jessica Purkiss|Videos 15 Civilia...|    1|Videos 15 Civilia...|\n",
      "|  4|Iranian woman jai...|      Howard Portnoy|Print An Iranian ...|    1|Print An Iranian ...|\n",
      "|  5|Jackie Mason: Hol...|     Daniel Nussbaum|In these trying t...|    0|In these trying t...|\n",
      "|  6|Life: Life Of Lux...|                null|Ever wonder how B...|    1|Ever wonder how B...|\n",
      "|  7|Benoît Hamon Wins...|     Alissa J. Rubin|PARIS  —   France...|    0|PARIS     France ...|\n",
      "|  8|Excerpts From a D...|                null|Donald J. Trump i...|    0|Donald J Trump is...|\n",
      "|  9|A Back-Channel Pl...|Megan Twohey and ...|A week before Mic...|    0|A week before Mic...|\n",
      "| 10|Obama’s Organizin...|         Aaron Klein|Organizing for Ac...|    0|Organizing for Ac...|\n",
      "| 11|\"BBC Comedy Sketc...|     Chris Tomlinson|The BBC produced ...|    0|The BBC produced ...|\n",
      "| 12|Russian Researche...|       Amando Flavio|The mystery surro...|    1|The mystery surro...|\n",
      "| 13|US Officials See ...|          Jason Ditz|Clinton Campaign ...|    1|Clinton Campaign ...|\n",
      "| 14|Re: Yes There Are...|        AnotherAnnie|Yes There Are Pai...|    1|Yes There Are Pai...|\n",
      "| 15|In Major League S...|       Jack Williams|Guillermo Barros ...|    0|Guillermo Barros ...|\n",
      "| 16|Wells Fargo Chief...|Michael Corkery a...|The scandal engul...|    0|The scandal engul...|\n",
      "| 17|Anonymous Donor P...|            Starkman|A Caddo Nation tr...|    1|A Caddo Nation tr...|\n",
      "| 18|FBI Closes In On ...|             The Doc|FBI Closes In On ...|    1|FBI Closes In On ...|\n",
      "| 19|Chuck Todd: ’Buzz...|           Jeff Poor|Wednesday after  ...|    0|Wednesday after  ...|\n",
      "+---+--------------------+--------------------+--------------------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# remove punctuation\n",
    "punctuation = string.punctuation\n",
    "punctuation += '—'\n",
    "\n",
    "def remove_punc(x):\n",
    "    new_str = x\n",
    "    for ch in punctuation:\n",
    "        new_str = new_str.replace(ch, '')\n",
    "    return new_str\n",
    "\n",
    "rp_udf = udf(lambda x: remove_punc(x))\n",
    "\n",
    "data1 = data.withColumn(\"text_cleaned\", rp_udf(col('text')))\n",
    "data1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1858e0bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "RegexTokenizer.__init__() got an unexpected keyword argument 'inputCol'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# # tokenizer\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mRegexTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputCol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext_cleaned\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputCol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m data2 \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtransform(data1)\n\u001b[1;32m      4\u001b[0m data2\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pyspark/__init__.py:139\u001b[0m, in \u001b[0;36mkeyword_only.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMethod \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m forces keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: RegexTokenizer.__init__() got an unexpected keyword argument 'inputCol'"
     ]
    }
   ],
   "source": [
    "# # tokenizer\n",
    "tokenizer = RegexTokenizer(inputCol='text_cleaned', outputCol=\"tokens\")\n",
    "data2 = tokenizer.transform(data1)\n",
    "data2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774c43fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # remove stopwords\n",
    "# remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"text_rm_stop\")\n",
    "# data1 = remover.transform(data1)\n",
    "# data1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb33b4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatizer = LemmatizerModel.pretrained(name=\"lemma_antbnc\", lang=\"en\")\n",
    "#         .setInputCol('text_rm_stop')\n",
    "#         .setOutputCol('lemma')\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4c9893",
   "metadata": {},
   "source": [
    "### Build Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abe702fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'JavaPackage' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mRegexTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \\\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;241m.\u001b[39msetInputCol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_cleaned\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;241m.\u001b[39msetOutputCol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m stopwords \u001b[38;5;241m=\u001b[39m StopWordsRemover() \\\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;241m.\u001b[39msetInputCol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;241m.\u001b[39msetOutputCol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_rm_stop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m lemmatizer \u001b[38;5;241m=\u001b[39m LemmatizerModel\u001b[38;5;241m.\u001b[39mpretrained(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlemma_antbnc\u001b[39m\u001b[38;5;124m\"\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;241m.\u001b[39msetInputCol(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext_rm_stop\u001b[39m\u001b[38;5;124m'\u001b[39m) \\\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;241m.\u001b[39msetOutputCol(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlemma\u001b[39m\u001b[38;5;124m'\u001b[39m) \n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pyspark/__init__.py:139\u001b[0m, in \u001b[0;36mkeyword_only.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMethod \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m forces keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sparknlp/annotator/token/regex_tokenizer.py:90\u001b[0m, in \u001b[0;36mRegexTokenizer.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;129m@keyword_only\u001b[39m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 90\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mRegexTokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclassname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcom.johnsnowlabs.nlp.annotators.RegexTokenizer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setDefault(\n\u001b[1;32m     92\u001b[0m         inputCols\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocument\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     93\u001b[0m         outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregexToken\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     99\u001b[0m         preservePosition\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    100\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pyspark/__init__.py:139\u001b[0m, in \u001b[0;36mkeyword_only.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMethod \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m forces keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sparknlp/common/annotator_model.py:37\u001b[0m, in \u001b[0;36mAnnotatorModel.__init__\u001b[0;34m(self, classname, java_model)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m classname \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m java_model:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m_java_class_name \u001b[38;5;241m=\u001b[39m classname\n\u001b[0;32m---> 37\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_java_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclassname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m java_model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_from_java()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pyspark/ml/wrapper.py:86\u001b[0m, in \u001b[0;36mJavaWrapper._new_java_obj\u001b[0;34m(java_class, *args)\u001b[0m\n\u001b[1;32m     84\u001b[0m     java_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(java_obj, name)\n\u001b[1;32m     85\u001b[0m java_args \u001b[38;5;241m=\u001b[39m [_py2java(sc, arg) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[0;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjava_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mjava_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'JavaPackage' object is not callable"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexTokenizer() \\\n",
    "    .setInputCol(\"text_cleaned\") \\\n",
    "    .setOutputCol(\"tokens\")\n",
    "stopwords = StopWordsRemover() \\\n",
    "    .setInputCol(\"tokens\") \\\n",
    "    .setOutputCol(\"text_rm_stop\")\n",
    "lemmatizer = LemmatizerModel.pretrained(name=\"lemma_antbnc\", lang=\"en\") \\\n",
    "        .setInputCol('text_rm_stop') \\\n",
    "        .setOutputCol('lemma') \n",
    "nlpPipeline = Pipeline(stages=[tokenizer, stopwords, lemmatizer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31552ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = nlpPipeline.fit(data1).transform(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c40050a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.15:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1119d99c0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0f7f5a",
   "metadata": {},
   "source": [
    "### Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbba263",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    " \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lems(x):\n",
    "    words = [lemmatizer.lemmatize(word) for word in x]\n",
    "    return words\n",
    "\n",
    "lem_udf = udf(lambda x: lems(x))\n",
    "\n",
    "final_df = final_df.withColumn(\"lem\", lem_udf(col('text_rm_stop')))\n",
    "final_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88de5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df.select(split(col(\"lem\"),\",\").alias(\"tokens\"), col(\"id\"), col(\"label\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956b1ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7a67cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4451ed8f",
   "metadata": {},
   "source": [
    "### Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1c48c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict(num_topics = 10,\n",
    "    iterations = 10,\n",
    "    vocabsize = 5000,\n",
    "    minDF = 0.02,\n",
    "    maxDF = 0.8\n",
    " )\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0156a823",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_vectorizer(inputCol, outputCol, params):\n",
    "    cv = CountVectorizer(\n",
    "        inputCol=inputCol,\n",
    "        outputCol=outputCol,\n",
    "        vocabSize=params['vocabsize'],\n",
    "        minDF=params['minDF'],\n",
    "        maxDF=params['maxDF'],\n",
    "        minTF=1.0\n",
    "    )\n",
    "    return cv\n",
    "\n",
    "\n",
    "def idf(inputCol, outputCol):\n",
    "    return IDF(inputCol=\"features\", outputCol=\"idf\")\n",
    "\n",
    "\n",
    "def lda(params):\n",
    "    lda = LDA(\n",
    "        k=params['num_topics'],\n",
    "        maxIter=params['iterations'],\n",
    "        optimizer=\"online\",\n",
    "        seed=1,\n",
    "        learningOffset=100.0,  # If high, early iterations are downweighted during training\n",
    "        learningDecay=0.75,    # Set between [0.5, 1) to guarantee asymptotic convergence\n",
    "    )\n",
    "    return lda\n",
    "\n",
    "\n",
    "def ml_pipeline(nlpPipelineDF, params):\n",
    "    \"\"\"Create a Spark ML pipeline and transform the input NLP-transformed DataFrame \n",
    "       to produce a trained LDA topic model for the given data.\n",
    "    \"\"\"\n",
    "    ml_pipe = Pipeline(\n",
    "        stages=[\n",
    "            count_vectorizer(\"lem\", \"features\", params),\n",
    "            idf(\"features\", \"idf\"),\n",
    "            lda(params)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    model = ml_pipe.fit(final_df)\n",
    "    lda_model = model.stages[2]\n",
    "    \n",
    "    # Calculate upper bound on model perplexity\n",
    "    mlPipelineDF = model.transform(final_df)\n",
    "    lda_perplexity = lda_model.logPerplexity(final_df)\n",
    "    return model, lda_perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206d55e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count Vectorizer\n",
    "model, lda_perplexity = ml_pipeline(final_df, params)\n",
    "print(\"Upper bound on perplexity: {}\".format(lda_perplexity))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
